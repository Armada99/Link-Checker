{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Link Checker",
      "provenance": [],
      "authorship_tag": "ABX9TyOsgpd5Idr99bT/OCdDmW7P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armada99/Link-Checker/blob/main/Link_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5T5Y-0N4JSo"
      },
      "source": [
        "Thanks to Coder (for coding it obviously)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmshys6fDS_x"
      },
      "source": [
        "# Run this cell first to export all links \n",
        "import threading\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://reddit.com/r/FREEMEDIAHECKYEAH/wiki/\"\n",
        "names = [\n",
        "            \"adblock-vpn-privacy\",\n",
        "            \"video\",\n",
        "            \"audio\",\n",
        "            \"games\",\n",
        "            \"reading\",\n",
        "            \"android\",\n",
        "            \"download\",\n",
        "            \"torrent\",\n",
        "            \"edu\",\n",
        "            \"tools-misc\",\n",
        "            \"misc\",\n",
        "            \"non-eng\",\n",
        "            \"storage\",\n",
        "            \"img-tools\"\n",
        "        ]\n",
        "\n",
        "with open(\"links.txt\", \"w\") as f:\n",
        "    f.write(\"\")\n",
        "\n",
        "def get_url(n):\n",
        "    finalurl = url + n + \".json\"\n",
        "    resp = requests.get(\n",
        "            finalurl, \n",
        "            headers = {\"user-agent\": \"fmhy\"}\n",
        "            )\n",
        "    r = resp.json()\n",
        "    soup = BeautifulSoup(html.unescape(r[\"data\"][\"content_html\"]), 'html.parser')\n",
        "\n",
        "    print(\"saving links from \" + n + \" to links.txt\")\n",
        "    with open(\"links.txt\", \"a\") as f:\n",
        "        for link in soup.find_all(\"a\"):\n",
        "            if \"http\" in link.get(\"href\"):\n",
        "                f.write(link.get(\"href\") + \"\\n\")\n",
        "\n",
        "threads = []\n",
        "for name in names:\n",
        "    locals()[name] = threading.Thread(target=get_url, args=(name,))\n",
        "    threads.append(locals()[name])\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wLMFwftyvo5",
        "outputId": "f90ffd2f-efd8-460e-9609-0140756b9512",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import requests\n",
        "import threading\n",
        "\n",
        "with open(\"links.txt\", \"r\") as f:\n",
        "    links = list(f)\n",
        "links_clean = set(links)\n",
        "links = list(links_clean)\n",
        "with open(\"status.txt\", \"w\") as f:\n",
        "    f.write(\"\")\n",
        "\n",
        "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"}\n",
        "Urls = []\n",
        "def link_checker(link):\n",
        "    try:\n",
        "        r = requests.get(link.strip(), timeout=1.0, headers=headers)\n",
        "        if r.status_code != 200: Urls.append(f\"{r.status_code} {r.url}\\n\")\n",
        "    except Exception as e:\n",
        "        Urls.append(f\"error {link.strip()}\\n\")\n",
        "        pass\n",
        "\n",
        "\n",
        "threads = []\n",
        "for i, link in enumerate(links):\n",
        "    threads.append(threading.Thread(target=link_checker, args=(link,)))\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()\n",
        "\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "with open(\"status.txt\", \"a\") as f:\n",
        "    for url in Urls:\n",
        "        f.write(url)\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    }
  ]
}