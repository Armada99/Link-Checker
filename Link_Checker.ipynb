{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Link Checker",
      "provenance": [],
      "authorship_tag": "ABX9TyM8b3d3y6iyM8x32ylDsDH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Armada99/Link-Checker/blob/main/Link_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmshys6fDS_x"
      },
      "source": [
        "import threading\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://reddit.com/r/FREEMEDIAHECKYEAH/wiki/\"\n",
        "names = [\n",
        "            \"adblock-vpn-privacy\",\n",
        "            \"video\",\n",
        "            \"audio\",\n",
        "            \"games\",\n",
        "            \"reading\",\n",
        "            \"android\",\n",
        "            \"download\",\n",
        "            \"torrent\",\n",
        "            \"edu\",\n",
        "            \"tools-misc\",\n",
        "            \"misc\",\n",
        "            \"non-eng\",\n",
        "            \"storage\",\n",
        "            \"img-tools\"\n",
        "        ]\n",
        "\n",
        "with open(\"links.txt\", \"w\") as f:\n",
        "    f.write(\"\")\n",
        "\n",
        "def get_url(n):\n",
        "    finalurl = url + n + \".json\"\n",
        "    resp = requests.get(\n",
        "            finalurl, \n",
        "            headers = {\"user-agent\": \"fmhy\"}\n",
        "            )\n",
        "    r = resp.json()\n",
        "    soup = BeautifulSoup(html.unescape(r[\"data\"][\"content_html\"]), 'html.parser')\n",
        "\n",
        "    print(\"saving links from \" + n + \" to links.txt\")\n",
        "    with open(\"links.txt\", \"a\") as f:\n",
        "        for link in soup.find_all(\"a\"):\n",
        "            if \"http\" in link.get(\"href\"):\n",
        "                f.write(link.get(\"href\") + \"\\n\")\n",
        "\n",
        "threads = []\n",
        "for name in names:\n",
        "    locals()[name] = threading.Thread(target=get_url, args=(name,))\n",
        "    threads.append(locals()[name])\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JurzZuBgGVH-"
      },
      "source": [
        "import requests\n",
        "import threading\n",
        "\n",
        "with open(\"links.txt\", \"r\") as f:\n",
        "    links = list(f)\n",
        "links_clean = set(links)\n",
        "links = list(links_clean)\n",
        "with open(\"status.txt\", \"w\") as f:\n",
        "    f.write(\"\")\n",
        "\n",
        "Urls = []\n",
        "def link_checker(link):\n",
        "    try:\n",
        "        r = requests.get(link.strip(), timeout=1.0)\n",
        "        Urls.append(f\"{r.status_code if r.status_code != 200 else ''} {r.url}\\n\")\n",
        "    except Exception as e:\n",
        "        Urls.append(f\"error {link.strip()}\\n\")\n",
        "        pass\n",
        "\n",
        "\n",
        "        \n",
        "    \n",
        "threads = []\n",
        "for i, link in enumerate(links):\n",
        "    threads.append(threading.Thread(target=link_checker, args=(link,)))\n",
        "\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()\n",
        "\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "with open(\"status.txt\", \"a\") as f:\n",
        "    for url in Urls:\n",
        "        f.write(url)\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}